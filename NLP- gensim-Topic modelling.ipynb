{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QefmSpaReELg"
   },
   "source": [
    "# **Topic Modelling **\n",
    "\n",
    "Latent Dirichlet Allocation or LDA is an algorithm that is used in topic modelling.\n",
    "\n",
    "Dirichlet process is a probability distribution whose range is a collection of probability distributions.\n",
    "\n",
    "'Dirichlet' indicates that LDA assumes that the distribution of topics in a document and the distribution of words in topics are both Dirichlet distributions.\n",
    "\n",
    "LDA is used to classify text in a document at a particular topic. \n",
    "\n",
    "Biulds a topic/doc model and words/topic model.\n",
    "\n",
    "Each doc is modeled as a multinomial distribution of topics and each topic is modeled as a multinomial distribution of words.\n",
    "\n",
    "LDA assumes every chunck of text and feed it into words that is related.\n",
    "\n",
    "Assumes that documents are produced from a mixture of topics. Those topics then generate words based on their probability of distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAgpvkqAhTa1"
   },
   "source": [
    "## **Context on the dataset.**\n",
    "\n",
    "The dataset is a collection newsgroup documents. The 20 newsgroups collection has become a popular data set for experiments in text format. \n",
    "\n",
    "To this dataset machine learning techniques, such as text classification and text clustering are applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KU5087tDeD3C"
   },
   "source": [
    "### **Importing the in-built dataset (fetch_20newsgroups) from the sklearn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Nrisks3yavgk"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpMR9gCgeACb"
   },
   "source": [
    "### **Importing warnings to ignore the warning messages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9z9lwQUka1q7"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L44vOJY6gvxu"
   },
   "source": [
    "### **Shuffling and splitting into train and test**\n",
    "\n",
    "This dataset already categorized into key topics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RKkWgNQsa2AI"
   },
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train',shuffle = True)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nk8txGj921OH"
   },
   "source": [
    "### **Listing the names of the 20Newsgrops**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sy-lw1Dda2Te",
    "outputId": "cc30c690-e1d8-4d96-9850-ba1e11c95938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(list(newsgroups_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mBjN2vq_a2oo",
    "outputId": "3ffde919-c270-43fb-f1dc-4d0b1bce2bf4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\",\n",
       " \"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3GcwhIcIg4e8",
    "outputId": "b0974229-bb8a-44eb-c783-e51711acc9c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbLHDPYYhQ3i",
    "outputId": "b6b4024b-5ba6-4cb8-fff3-bdde3439eda9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMlKQcV4AFfG"
   },
   "source": [
    "### **Importing the requried libraries.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czf6NxI3IPxy"
   },
   "source": [
    "**Gensim** processes the raw unstructured texts using unsupervised machine laerning algorithim.\n",
    "\n",
    "**gensim.utils.simple_preprocess** - coverts document into a list of tokens.\n",
    "\n",
    "**gensim.parsing.preprocessing** - process of converting the data into required format for further processing. \n",
    "Here the data is texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fk7DeZ3ba292"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdVnsSZSMw9e"
   },
   "source": [
    "### **Wordnet**\n",
    "\n",
    "Wordnet is a large word database of English Noun, Adjectives,Adverb and verbs.\n",
    "\n",
    "Lexical Database used for word-sense disambiguation, information retrieval, automatic text classification and machine translation.\n",
    "\n",
    "The most important uses of WordNet is to find out the similarity among words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eBmpvMnkbW2_",
    "outputId": "a593e510-2bbe-4780-8548-7b7de3cde046"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGu9ojwo4Rk9"
   },
   "source": [
    "### **Importing the stem.porter**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AItfiKfwbWzb"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer,LancasterStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HM6UdWH3bWwE",
    "outputId": "78b4781c-32ec-417a-8ebc-b166a81f4ea8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4M5O3gS_bWr4",
    "outputId": "6b7504af-df54-4504-a89a-709c36b1f60a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6I7yW6-o5oLE"
   },
   "source": [
    "### **WordNetLemmatizer()**\n",
    "\n",
    "NLTK WordNet Lemmatizer for a Part-of-Speech tagging project by first modifying each word in the training corpus to its stem (in place modification), and then training only on the new corpus.\n",
    "\n",
    "\n",
    "WordNetLemmatizer().lemmatize('word', pos = 'v')\n",
    "\n",
    "pos: parts of speech. Here its Verb (v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRuJmf7tbWod",
    "outputId": "be410df6-5b88-4433-eb84-d2e79ce7df88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos = 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mytiJoj63Yp"
   },
   "source": [
    "### **SnowballStemmer()**\n",
    "\n",
    "Snowball Stemmer is the improvised version of porter stemmer.\n",
    "\n",
    "Snowball is Stemmed to more accurate root word than the porter \n",
    "\n",
    "Eg: \n",
    "\n",
    "    cared ----> care\n",
    "    university ----> univers\n",
    "    fairly ----> fair\n",
    "    easily ----> easili\n",
    "    singing ----> sing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5w-5t17cbWkM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HV4_9qModq6r"
   },
   "source": [
    "### **Throwing some words and fetching the snowball stemmer performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ElKKWB4cbWg8"
   },
   "outputs": [],
   "source": [
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned',\n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational',\n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "jqozfvSQbWeQ",
    "outputId": "f81e24b6-443e-40fb-c83b-1cbf5afe3a85"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-69f50f85-f0ff-40e3-8122-412e9a36768c\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69f50f85-f0ff-40e3-8122-412e9a36768c')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-69f50f85-f0ff-40e3-8122-412e9a36768c button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-69f50f85-f0ff-40e3-8122-412e9a36768c');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data={'original word':original_words, 'stemmed':singles })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Veiain38TIJ"
   },
   "source": [
    "### **Lancaster Stemmer**\n",
    "\n",
    "Add our own custom rules in this algorithm when we implement this using the NLTK package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-LG8M9ZAAlzF"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import *\n",
    "\n",
    "p_stemmer=LancasterStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bXOXcS_6Al3u"
   },
   "outputs": [],
   "source": [
    "original_words = ['carried', 'flew', 'died', 'mutes', 'denial', 'greedy', 'ownered',\n",
    "           'humbling','greeting', 'stated', 'idealism','sensational',\n",
    "           'traditional', 'reckless', 'condemnation','blended','beneficiary','notabily','materialistic']\n",
    "\n",
    "singles = [stemmer.stem(verb) for verb in original_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "BKAMllD6AmUa",
    "outputId": "dacd8267-2772-41b1-907a-36127aab0875"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-635d57e5-cdeb-4647-ab7f-6e43bef5aed8\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>carried</td>\n",
       "      <td>carri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flew</td>\n",
       "      <td>flew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mutes</td>\n",
       "      <td>mute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denial</td>\n",
       "      <td>denial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>greedy</td>\n",
       "      <td>greedi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ownered</td>\n",
       "      <td>owner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>humbling</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>greeting</td>\n",
       "      <td>greet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stated</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>idealism</td>\n",
       "      <td>ideal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>reckless</td>\n",
       "      <td>reckless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>condemnation</td>\n",
       "      <td>condemn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>blended</td>\n",
       "      <td>blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>beneficiary</td>\n",
       "      <td>beneficiari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>notabily</td>\n",
       "      <td>notabili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>materialistic</td>\n",
       "      <td>materialist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-635d57e5-cdeb-4647-ab7f-6e43bef5aed8')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-635d57e5-cdeb-4647-ab7f-6e43bef5aed8 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-635d57e5-cdeb-4647-ab7f-6e43bef5aed8');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "    original word      stemmed\n",
       "0         carried        carri\n",
       "1            flew         flew\n",
       "2            died          die\n",
       "3           mutes         mute\n",
       "4          denial       denial\n",
       "5          greedy       greedi\n",
       "6         ownered        owner\n",
       "7        humbling        humbl\n",
       "8        greeting        greet\n",
       "9          stated        state\n",
       "10       idealism        ideal\n",
       "11    sensational       sensat\n",
       "12    traditional       tradit\n",
       "13       reckless     reckless\n",
       "14   condemnation      condemn\n",
       "15        blended        blend\n",
       "16    beneficiary  beneficiari\n",
       "17       notabily     notabili\n",
       "18  materialistic  materialist"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data={'original word':original_words, 'stemmed':singles})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwyyE0gMW44i"
   },
   "source": [
    "### **Stemming and Lemmatizing the text get verb.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "A2OqaKXPbWac"
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ve2oKaOXUo3"
   },
   "source": [
    "## **Tokenize and lemmatize**\n",
    "\n",
    "Creating the function for preprocessing the text \n",
    "\n",
    "checking whether the token in gensim_utils,but not in gensim parsing and tokens length greater than 3\n",
    "\n",
    "Append it with lemmatize_stemming(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bgTYCGZUbWXH"
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3TcbWbhiLTv"
   },
   "source": [
    "### **Choosing the 20th document and taking a sentence from it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "60ukjrdzbWS7"
   },
   "outputs": [],
   "source": [
    "document_num = 20\n",
    "doc_sample = 'This disk has failed many times. I would like to get it replaced.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vil0O9oEbpl2",
    "outputId": "fd9e7846-895a-429c-a159-5d6fdaecbed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n"
     ]
    }
   ],
   "source": [
    "print(\"Original document: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJMWpLWWZF4j"
   },
   "source": [
    "### **Spliting the sentence into words (Tokeniztion)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uq-6QujJbphV",
    "outputId": "a5ae2a0e-72ca-44d2-9e27-4088bc4455e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'disk', 'has', 'failed', 'many', 'times.', 'I', 'would', 'like', 'to', 'get', 'it', 'replaced.']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "  words.append(word)\n",
    "\n",
    "print(words)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wc88mRN-bpfJ",
    "outputId": "9947dd0f-7aa0-4bf9-98c0-2d73a5349f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['disk', 'fail', 'time', 'like', 'replac']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMg5oMKXZR83"
   },
   "source": [
    "### **Preprocessing the documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1qmsosmbpcn",
    "outputId": "c0a98aa6-6c2d-40a1-e6ab-778066c0d3f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lerxst', 'thing', 'subject', 'nntp', 'post', 'host', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'wonder', 'enlighten', 'door', 'sport', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'histori', 'info', 'funki', 'look', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst'], ['guykuo', 'carson', 'washington', 'subject', 'clock', 'poll', 'final', 'summari', 'final', 'clock', 'report', 'keyword', 'acceler', 'clock', 'upgrad', 'articl', 'shelley', 'qvfo', 'innc', 'organ', 'univers', 'washington', 'line', 'nntp', 'post', 'host', 'carson', 'washington', 'fair', 'number', 'brave', 'soul', 'upgrad', 'clock', 'oscil', 'share', 'experi', 'poll', 'send', 'brief', 'messag', 'detail', 'experi', 'procedur', 'speed', 'attain', 'rat', 'speed', 'card', 'adapt', 'heat', 'sink', 'hour', 'usag', 'floppi', 'disk', 'function', 'floppi', 'especi', 'request', 'summar', 'day', 'network', 'knowledg', 'base', 'clock', 'upgrad', 'haven', 'answer', 'poll', 'thank', 'guykuo', 'washington']]\n"
     ]
    }
   ],
   "source": [
    "processed_docs = []\n",
    " \n",
    "for doc in newsgroups_train.data:\n",
    "    processed_docs.append(preprocess(doc))\n",
    " \n",
    "print(processed_docs[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k13AmkiDZtay"
   },
   "source": [
    "### **Defining the Dictionary and corpus**\n",
    "\n",
    "### **Corpora.Dictionary()**\n",
    "\n",
    "implements the concept of a Dictionary – a mapping between words and their integer ids.\n",
    "\n",
    "Dictionary encapsulates the mapping between normalized words and their integer ids.\n",
    "\n",
    "Here the mappin is performed on the processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "mrjc4ShMbpZV"
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY66Y33pbf3C"
   },
   "source": [
    "### **Counting 100 words in the dictionary of processed document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8xx31qDbpWJ",
    "outputId": "da40186b-172e-4263-85e4-66b9c20eef30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 addit\n",
      "1 bodi\n",
      "2 bricklin\n",
      "3 bring\n",
      "4 bumper\n",
      "5 call\n",
      "6 colleg\n",
      "7 door\n",
      "8 earli\n",
      "9 engin\n",
      "10 enlighten\n",
      "11 funki\n",
      "12 histori\n",
      "13 host\n",
      "14 info\n",
      "15 know\n",
      "16 late\n",
      "17 lerxst\n",
      "18 line\n",
      "19 look\n",
      "20 mail\n",
      "21 maryland\n",
      "22 model\n",
      "23 neighborhood\n",
      "24 nntp\n",
      "25 organ\n",
      "26 park\n",
      "27 post\n",
      "28 product\n",
      "29 rest\n",
      "30 separ\n",
      "31 small\n",
      "32 spec\n",
      "33 sport\n",
      "34 subject\n",
      "35 tellm\n",
      "36 thank\n",
      "37 thing\n",
      "38 univers\n",
      "39 wonder\n",
      "40 year\n",
      "41 acceler\n",
      "42 adapt\n",
      "43 answer\n",
      "44 articl\n",
      "45 attain\n",
      "46 base\n",
      "47 brave\n",
      "48 brief\n",
      "49 card\n",
      "50 carson\n",
      "51 clock\n",
      "52 day\n",
      "53 detail\n",
      "54 disk\n",
      "55 especi\n",
      "56 experi\n",
      "57 fair\n",
      "58 final\n",
      "59 floppi\n",
      "60 function\n",
      "61 guykuo\n",
      "62 haven\n",
      "63 heat\n",
      "64 hour\n",
      "65 innc\n",
      "66 keyword\n",
      "67 knowledg\n",
      "68 messag\n",
      "69 network\n",
      "70 number\n",
      "71 oscil\n",
      "72 poll\n",
      "73 procedur\n",
      "74 qvfo\n",
      "75 rat\n",
      "76 report\n",
      "77 request\n",
      "78 send\n",
      "79 share\n",
      "80 shelley\n",
      "81 sink\n",
      "82 soul\n",
      "83 speed\n",
      "84 summar\n",
      "85 summari\n",
      "86 upgrad\n",
      "87 usag\n",
      "88 washington\n",
      "89 access\n",
      "90 activ\n",
      "91 actual\n",
      "92 advanc\n",
      "93 anybodi\n",
      "94 anymor\n",
      "95 appear\n",
      "96 better\n",
      "97 breifli\n",
      "98 bunch\n",
      "99 convict\n",
      "100 corner\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 100:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UK0aDmoMZ4v5"
   },
   "source": [
    "### **dictionary.filter_extremes()**\n",
    "\n",
    "Filter out tokens in the dictionary by their frequency.\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "**no_below** (int, optional) – Keep tokens which are contained in at least no_below documents.\n",
    "\n",
    "**no_above** (float, optional) – Keep tokens which are contained in no more than no_above documents (fraction of total corpus size, not an absolute number).\n",
    "\n",
    "**keep_n** (int, optional) – Keep only the first keep_n most frequent tokens.\n",
    "\n",
    "**keep_tokens** (iterable of str) – Iterable of tokens that must stay in dictionary after filtering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "dZbDOmA4bpSz"
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below = 15, no_above = 0.1, keep_n = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFVJMTBCZ7XC"
   },
   "source": [
    "### **Frequency of the Words in the documents.**\n",
    "\n",
    "**doc2bow(doc)**- Convert document into the bag-of-words (BoW)\n",
    "\n",
    " format = list of (token_id, token_count) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sd5DyP8EbpL7",
    "outputId": "cdf7b932-9f06-40cd-b38f-0541dc1c45c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 18 (\"rest\") appears 1 time.\n",
      "Word 166 (\"clear\") appears 1 time.\n",
      "Word 336 (\"refer\") appears 1 time.\n",
      "Word 350 (\"true\") appears 1 time.\n",
      "Word 391 (\"technolog\") appears 1 time.\n",
      "Word 437 (\"christian\") appears 1 time.\n",
      "Word 453 (\"exampl\") appears 1 time.\n",
      "Word 476 (\"jew\") appears 1 time.\n",
      "Word 480 (\"lead\") appears 1 time.\n",
      "Word 482 (\"littl\") appears 3 time.\n",
      "Word 520 (\"wors\") appears 2 time.\n",
      "Word 721 (\"keith\") appears 3 time.\n",
      "Word 732 (\"punish\") appears 1 time.\n",
      "Word 803 (\"california\") appears 1 time.\n",
      "Word 859 (\"institut\") appears 1 time.\n",
      "Word 917 (\"similar\") appears 1 time.\n",
      "Word 990 (\"allan\") appears 1 time.\n",
      "Word 991 (\"anti\") appears 1 time.\n",
      "Word 992 (\"arriv\") appears 1 time.\n",
      "Word 993 (\"austria\") appears 1 time.\n",
      "Word 994 (\"caltech\") appears 2 time.\n",
      "Word 995 (\"distinguish\") appears 1 time.\n",
      "Word 996 (\"german\") appears 1 time.\n",
      "Word 997 (\"germani\") appears 3 time.\n",
      "Word 998 (\"hitler\") appears 1 time.\n",
      "Word 999 (\"livesey\") appears 2 time.\n",
      "Word 1000 (\"motto\") appears 2 time.\n",
      "Word 1001 (\"order\") appears 1 time.\n",
      "Word 1002 (\"pasadena\") appears 1 time.\n",
      "Word 1003 (\"pompous\") appears 1 time.\n",
      "Word 1004 (\"popul\") appears 1 time.\n",
      "Word 1005 (\"rank\") appears 1 time.\n",
      "Word 1006 (\"schneider\") appears 1 time.\n",
      "Word 1007 (\"semit\") appears 1 time.\n",
      "Word 1008 (\"social\") appears 1 time.\n",
      "Word 1009 (\"solntz\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    " \n",
    "document_num = 20\n",
    "bow_doc_x = bow_corpus[document_num]\n",
    " \n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "                                                     dictionary[bow_doc_x[i][0]], \n",
    "                                                     bow_doc_x[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0xpmSHDajCs"
   },
   "source": [
    "### **Model Building**\n",
    "\n",
    "LDA (Latent Dirichlet Allocation) is a kind of unsupervised method to classify documents by topic number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "yhK5evJwb8Fl"
   },
   "outputs": [],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus,\n",
    "                                   num_topics = 8,\n",
    "                                   id2word = dictionary,                                   \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICAWOz6BgK79"
   },
   "source": [
    "### **Displaying the topic distribution of the first document.**\n",
    "\n",
    "“idx” simply means “index”, the position of the element you are currently accessing\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "**chunksize** - number of documents to be used in each training chunk.\n",
    "\n",
    " **update_every**  - determines how often the model parameters should be updated and passes is the total number of training passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JIxaJLdxb8Ja",
    "outputId": "fba12c57-518e-4304-afa7-565f7e8b3f04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.012*\"armenian\" + 0.006*\"turkish\" + 0.005*\"bike\" + 0.005*\"leav\" + 0.004*\"road\" + 0.004*\"home\" + 0.004*\"turk\" + 0.003*\"greek\" + 0.003*\"armenia\" + 0.003*\"game\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.021*\"drive\" + 0.007*\"control\" + 0.007*\"disk\" + 0.006*\"hard\" + 0.004*\"caus\" + 0.004*\"pitt\" + 0.004*\"effect\" + 0.003*\"gordon\" + 0.003*\"firearm\" + 0.003*\"price\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.008*\"wire\" + 0.008*\"scsi\" + 0.008*\"power\" + 0.005*\"data\" + 0.004*\"design\" + 0.004*\"grind\" + 0.004*\"circuit\" + 0.004*\"chip\" + 0.004*\"engin\" + 0.004*\"connect\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.012*\"space\" + 0.010*\"nasa\" + 0.006*\"presid\" + 0.005*\"nation\" + 0.005*\"research\" + 0.005*\"program\" + 0.004*\"center\" + 0.004*\"orbit\" + 0.004*\"health\" + 0.004*\"group\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.010*\"govern\" + 0.009*\"encrypt\" + 0.007*\"secur\" + 0.007*\"israel\" + 0.006*\"chip\" + 0.006*\"public\" + 0.006*\"clipper\" + 0.006*\"isra\" + 0.004*\"protect\" + 0.004*\"key\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.019*\"game\" + 0.016*\"team\" + 0.012*\"play\" + 0.010*\"player\" + 0.009*\"hockey\" + 0.008*\"toronto\" + 0.006*\"season\" + 0.006*\"canada\" + 0.005*\"score\" + 0.005*\"leagu\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.016*\"window\" + 0.014*\"file\" + 0.010*\"program\" + 0.007*\"card\" + 0.007*\"version\" + 0.006*\"softwar\" + 0.006*\"imag\" + 0.005*\"graphic\" + 0.005*\"avail\" + 0.005*\"color\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.010*\"christian\" + 0.006*\"jesus\" + 0.006*\"exist\" + 0.004*\"claim\" + 0.004*\"human\" + 0.004*\"moral\" + 0.004*\"jew\" + 0.004*\"word\" + 0.004*\"religion\" + 0.004*\"life\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpZ4nLrld-GH"
   },
   "source": [
    "## **Conclusion**\n",
    "\n",
    "Thus the Topic modelling has clustered words to relative groups or topics.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
